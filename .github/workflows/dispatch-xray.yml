  name: Xray Dispatch Trigger from Jira

  on:
    workflow_dispatch:
      inputs:
        testExecKey:
          description: 'Test Exec ID from Xray'
          default: ''
          required: true
          type: string
        projectKey:
          description: 'Project ID from Xray'
          default: ''
          required: true
          type: string

  jobs:
    xray-to-databricks:
      runs-on: ubuntu-latest

      steps:
        - name: Checkout Repo
          uses: actions/checkout@v3

        - name: Set up Python
          uses: actions/setup-python@v4
          with:
            python-version: '3.x'

        - name: Install Required Tools
          run: |
            pip install requests

        - name: Debug Event
          run: |
            echo "Received event: ${{ github.event_name }}"
            echo "Test Execution Key: ${{ github.event.inputs.testExecKey }}"
            echo "Project Key: ${{ github.event.inputs.projectKey }}"

        - name: Clear Existing Feature Files
          run: |
            echo "Cleaning up existing feature files..."
            rm -rf ./src/main/resources/features/* || true

        - name: Pull Feature Files from Xray (with file content-type check)
          run: |
            echo "Authenticating with Xray Cloud to get a Bearer token..."
            XRAY_CLIENT_ID="7FDC446FCBFE4B6BB03E72C3D2031C57"
            XRAY_CLIENT_SECRET="dc1c0fd13c58178eb36fa3a25bbeb401a4cffd9bea7b0c601bcda798ecc6de6d"
            AUTH_RESPONSE=$(curl -s -H "Content-Type: application/json" -X POST \
                --data "{\"client_id\": \"$XRAY_CLIENT_ID\", \"client_secret\": \"$XRAY_CLIENT_SECRET\"}" \
                "https://xray.cloud.getxray.app/api/v2/authenticate")
            XRAY_TOKEN=$(echo $AUTH_RESPONSE | tr -d '"')
            if [[ -z "$XRAY_TOKEN" ]]; then
                echo "‚ùå Failed to authenticate with Xray"
                exit 1
            fi
            mkdir -p ./src/main/resources/features
            echo "Downloading feature files..."
            curl -s -D headers.txt -o features.zip \
              -H "Authorization: Bearer $XRAY_TOKEN" \
              "https://xray.cloud.getxray.app/api/v2/export/cucumber?keys=${{ github.event.inputs.testExecKey }}"
            unzip -o features.zip -d ./src/main/resources/features/

        - name: Patch Feature Files with Test and Exec Tags
          run: |
            FEATURE_DIR=./src/main/resources/features
            TEST_EXEC_KEY=${{ github.event.inputs.testExecKey }}
  
            echo "üîß Patching feature files with @REQ_<exec> and @TEST_<case>..."
  
            for FILE in $(find "$FEATURE_DIR" -name "*.feature"); do
              TEST_KEY=$(basename "$FILE" | grep -oP 'RUD-\d+')
  
              if [[ -n "$TEST_KEY" ]]; then
                echo "üìÑ Processing $FILE"
  
                if ! grep -q "@REQ_$TEST_EXEC_KEY" "$FILE"; then
                  echo "üîó Inserting @REQ_$TEST_EXEC_KEY above Feature line"
                  sed -i "/^Feature:/i @REQ_$TEST_EXEC_KEY" "$FILE"
                fi
  
                awk -v testTag="@TEST_$TEST_KEY" '
                  BEGIN { skip = 0 }
                  /^@TEST_/ { skip = 1 }
                  /^Scenario/ {
                    if (!skip) print testTag
                    skip = 0
                  }
                  { print }
                ' "$FILE" > tmp && mv tmp "$FILE"
  
              else
                echo "‚ö†Ô∏è Skipping $FILE ‚Äì test key not found in filename"
              fi
            done

        - name: Commit and Push Feature Files
          run: |
            git config user.name "github-actions"
            git config user.email "github-actions@github.com"
            git add src/main/resources/features/
            if git diff --cached --quiet; then
              echo "‚ÑπÔ∏è No changes to commit."
            else
              git commit -m "ü§ñ Auto-import feature files from Xray"
              git push origin feature/Rudra-Graph
            fi

        - name: Set Databricks Host and Token
          run: |
            echo "DATABRICKS_HOST=https://adb-2119706740354136.16.azuredatabricks.net" >> $GITHUB_ENV
            echo "DATABRICKS_TOKEN=dapif50e5c2c3b7f21974d4d64356224d524-3" >> $GITHUB_ENV

        - name: Set up Java 8
          uses: actions/setup-java@v3
          with:
            java-version: '8'
            distribution: 'temurin'

        - name: Build with Maven
          run: mvn clean package -DskipTests

        - name: List target directory (Debug)
          run: ls -lh target

        - name: Install Databricks CLI
          run: pip install databricks-cli

        - name: Generate Timestamped DBFS Folder
          id: timestamp
          run: |
            PATH_ID="${{ github.event.inputs.projectKey }}-$(date +%s)"
            echo "path=$PATH_ID" >> $GITHUB_OUTPUT
            echo "REPORT_PATH=/dbfs/SharedResults/$PATH_ID" >> $GITHUB_ENV

        - name: Create Databricks Cluster
          id: create-cluster
          run: |
            response=$(curl -s -X POST $DATABRICKS_HOST/api/2.0/clusters/create \
              -H "Authorization: Bearer $DATABRICKS_TOKEN" \
              -H "Content-Type: application/json" \
              -d '{
                "cluster_name": "CI-Automation-Cluster",
                "spark_version": "13.3.x-scala2.12",
                "node_type_id": "Standard_DS3_v2",
                "num_workers": 1
              }')
            cluster_id=$(echo "$response" | jq -r '.cluster_id')
            echo "::set-output name=cluster_id::$cluster_id"

        - name: Wait for Cluster to be RUNNING
          run: |
            for i in {1..10}; do
              status=$(curl -s -X GET "$DATABRICKS_HOST/api/2.0/clusters/get?cluster_id=${{ steps.create-cluster.outputs.cluster_id }}" \
                -H "Authorization: Bearer $DATABRICKS_TOKEN" | jq -r '.state')
              echo "Attempt $i: Cluster status = $status"
              if [[ "$status" == "RUNNING" ]]; then
                echo "‚úÖ Cluster is ready!"
                break
              elif [[ "$status" == "TERMINATED" || "$status" == "ERROR" ]]; then
                echo "‚ùå Cluster failed to start!"
                exit 1
              fi
              sleep 30
            done

        - name: Upload JAR to DBFS with retry
          run: |
            for attempt in {1..3}; do
              echo "Upload attempt $attempt..."
              databricks fs cp target/amida-databricks-1.0-SNAPSHOT-jar-with-dependencies.jar dbfs:/FileStore/jars/amida-databricks-1.0-SNAPSHOT-jar-with-dependencies.jar --overwrite && break
              echo "Upload failed, retrying in 10s..."
              sleep 10
            done

        - name: Trigger Job from Uploaded JAR
          id: trigger-job
          run: |
            run_response=$(curl -s -X POST $DATABRICKS_HOST/api/2.1/jobs/runs/submit \
              -H "Authorization: Bearer $DATABRICKS_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{
                \"run_name\": \"CI Run\",
                \"existing_cluster_id\": \"${{ steps.create-cluster.outputs.cluster_id }}\",
                \"spark_jar_task\": {
                  \"main_class_name\": \"runners.TestRunner\",
                  \"parameters\": [\"--REPORT_PATH\", \"${{ env.REPORT_PATH }}\"]
                },
                \"libraries\": [
                  { \"jar\": \"dbfs:/FileStore/jars/amida-databricks-1.0-SNAPSHOT-jar-with-dependencies.jar\" }
                ]
              }")
            run_id=$(echo "$run_response" | jq -r '.run_id')
            echo "::set-output name=run_id::$run_id"

        - name: Poll for Job Completion (max 2 mins)
          run: |
            for i in {1..4}; do
              status=$(curl -s -X GET "$DATABRICKS_HOST/api/2.1/jobs/runs/get?run_id=${{ steps.trigger-job.outputs.run_id }}" \
                -H "Authorization: Bearer $DATABRICKS_TOKEN" | jq -r '.state.life_cycle_state')
              echo "Attempt $i: Job status = $status"
              if [[ "$status" == "TERMINATED" || "$status" == "SKIPPED" || "$status" == "INTERNAL_ERROR" ]]; then
                break
              fi
              sleep 30
            done

        - name: Fetch Artifacts Back to GitHub (if available)
          run: |
            DBFS_PATH="/SharedResults/${{ steps.timestamp.outputs.path }}"
            echo "Attempting to download results from $DBFS_PATH..."
            for i in {1..5}; do
              databricks fs cp dbfs:$DBFS_PATH/cucumber.json ./cucumber.json --overwrite && break || sleep 10
            done

        - name: Upload Cucumber JSON to GitHub Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: cucumber-json
            path: ./cucumber.json
            retention-days: 7

        - name: Download Cucumber Artifact
          uses: actions/download-artifact@v4
          with:
            name: cucumber-json

        - name: Authenticate and Upload Results to Xray
          run: |
            XRAY_CLIENT_ID="2BE4EE591A1646FD95A726D05C81D7DC"
            XRAY_CLIENT_SECRET="f802202c0ab3e24dd4aa7db288168e5a0ae76d479d93fb049899cbf449b7ad95"
            JSON_FILE="cucumber.json"
  
            echo "üîê Getting Xray Bearer Token..."
            XRAY_TOKEN=$(curl -s -H "Content-Type: application/json" -X POST \
              --data "{\"client_id\": \"$XRAY_CLIENT_ID\", \"client_secret\": \"$XRAY_CLIENT_SECRET\"}" \
              https://xray.cloud.getxray.app/api/v2/authenticate | tr -d '"')
  
            if [[ -z "$XRAY_TOKEN" ]]; then
              echo "‚ùå Failed to authenticate. Exiting."
              exit 1
            else
              echo "‚úÖ Token acquired."
            fi
  
            echo "üì§ Uploading $JSON_FILE to Xray..."
            curl -H "Content-Type: application/json" \
                 -X POST \
                 -H "Authorization: Bearer $XRAY_TOKEN" \
                 --data "@$JSON_FILE" \
                 https://xray.cloud.getxray.app/api/v2/import/execution/cucumber

        - name: Cleanup SharedResults Directory (optional)
          if: success()
          run: |
            DBFS_PATH="/SharedResults/${{ steps.timestamp.outputs.path }}"
            echo "Cleaning up $DBFS_PATH..."
            databricks fs rm -r dbfs:$DBFS_PATH || echo "Nothing to clean."

        - name: Cleanup Feature Files and Push
          if: success()
          run: |
            echo "üßπ Cleaning up feature files and pushing to repo..."
            rm -rf ./src/main/resources/features/*
            git config user.name "github-actions"
            git config user.email "github-actions@github.com"
            git add src/main/resources/features/
            if git diff --cached --quiet; then
              echo "‚ÑπÔ∏è No changes to commit after cleanup."
            else
              git commit -m "üßº Cleanup: removed feature files after execution"
              git push origin feature/Rudra-Graph
            fi

        - name: Terminate Databricks Cluster
          if: always()
          run: |
            echo "üõë Terminating Databricks cluster..."
            curl -s -X POST "$DATABRICKS_HOST/api/2.0/clusters/delete" \
              -H "Authorization: Bearer $DATABRICKS_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{\"cluster_id\": \"${{ steps.create-cluster.outputs.cluster_id }}\"}"
