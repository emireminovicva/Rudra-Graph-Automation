name: Xray Dispatch Trigger from Jira

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  xray-to-databricks:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Required Tools
        run: |
          pip install requests

      - name: Debug Event
        run: |
          echo "Received event: ${{ github.event_name }}"
          echo "Test Execution Key: SDP-37817"
          echo "Project Key: SDP"

      - name: Clear Existing Feature Files
        run: |
          echo "Cleaning up existing feature files..."
          rm -rf ./src/main/resources/features/* || true

      - name: Pull Feature Files from Xray Server/DC
        run: |
          mkdir -p ./src/main/resources/features
          curl -H \"Authorization: Bearer KqYgtEnBjmKbGTXyc8idSTE505y4TaG3tF3Xxy\" \
               -X GET "https://jira.devops.va.gov/rest/raven/1.0/export/test?keys=SDP-37817" \
               -o ./src/main/resources/features/SDP-37817.feature

      - name: Patch Feature Files with Test and Exec Tags
        run: |
          FEATURE_DIR=./src/main/resources/features
          TEST_EXEC_KEY=SDP-37817

          echo "üîß Patching feature files with @REQ_<exec> and @TEST_<case>..."

          for FILE in $(find "$FEATURE_DIR" -name "*.feature"); do
            TEST_KEY=$(basename "$FILE" | grep -oP 'RUD-\d+')

            if [[ -n "$TEST_KEY" ]]; then
              echo "üìÑ Processing $FILE"

              if ! grep -q "@REQ_$TEST_EXEC_KEY" "$FILE"; then
                echo "üîó Inserting @REQ_$TEST_EXEC_KEY above Feature line"
                sed -i "/^Feature:/i @REQ_$TEST_EXEC_KEY" "$FILE"
              fi

              awk -v testTag="@TEST_$TEST_KEY" '
                BEGIN { skip = 0 }
                /^@TEST_/ { skip = 1 }
                /^Scenario/ {
                  if (!skip) print testTag
                  skip = 0
                }
                { print }
              ' "$FILE" > tmp && mv tmp "$FILE"

            else
              echo "‚ö†Ô∏è Skipping $FILE ‚Äì test key not found in filename"
            fi
          done

      - name: Commit and Push Feature Files
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git add src/main/resources/features/
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è No changes to commit."
          else
            git commit -m "ü§ñ Auto-import feature files from Xray"
            git push origin feature/Rudra-Graph

      - name: Set Databricks Host and Token
        run: |
          echo "DATABRICKS_HOST=https://adb-2119706740354136.16.azuredatabricks.net" >> $GITHUB_ENV
          echo "DATABRICKS_TOKEN=DATABRICKSTOKEN" >> $GITHUB_ENV

      - name: Set up Java 8
        uses: actions/setup-java@v3
        with:
          java-version: '8'
          distribution: 'temurin'

      - name: Build with Maven
        run: mvn clean package -DskipTests

      - name: List target directory (Debug)
        run: ls -lh target

      - name: Install Databricks CLI
        run: pip install databricks-cli

      - name: Generate Timestamped DBFS Folder
        id: timestamp
        run: |
          PATH_ID="sdp-$(date +%s)"
          echo "path=$PATH_ID" >> $GITHUB_OUTPUT
          echo "REPORT_PATH=/dbfs/SharedResults/$PATH_ID" >> $GITHUB_ENV

      - name: Create Databricks Cluster
        id: create-cluster
        run: |
          response=$(curl -s -X POST $DATABRICKS_HOST/api/2.0/clusters/create \
            -H "Authorization: Bearer $DATABRICKS_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "cluster_name": "CI-Automation-Cluster",
              "spark_version": "13.3.x-scala2.12",
              "node_type_id": "Standard_DS3_v2",
              "num_workers": 1
            }')
          cluster_id=$(echo "$response" | jq -r '.cluster_id')
          echo "::set-output name=cluster_id::$cluster_id"

      - name: Wait for Cluster to be RUNNING
        run: |
          for i in {1..10}; do
            status=$(curl -s -X GET "$DATABRICKS_HOST/api/2.0/clusters/get?cluster_id=${{ steps.create-cluster.outputs.cluster_id }}" \
              -H "Authorization: Bearer $DATABRICKS_TOKEN" | jq -r '.state')
            echo "Attempt $i: Cluster status = $status"
            if [[ "$status" == "RUNNING" ]]; then
              echo "‚úÖ Cluster is ready!"
              break
            elif [[ "$status" == "TERMINATED" || "$status" == "ERROR" ]]; then
              echo "‚ùå Cluster failed to start!"
              exit 1
            fi
            sleep 30
          done

      - name: Upload JAR to DBFS with retry
        run: |
          for attempt in {1..3}; do
            echo "Upload attempt $attempt..."
            databricks fs cp target/amida-databricks-1.0-SNAPSHOT-jar-with-dependencies.jar dbfs:/FileStore/jars/amida-databricks-1.0-SNAPSHOT-jar-with-dependencies.jar --overwrite && break
            echo "Upload failed, retrying in 10s..."
            sleep 10
          done

      - name: Trigger Job from Uploaded JAR
        id: trigger-job
        run: |
          run_response=$(curl -s -X POST $DATABRICKS_HOST/api/2.1/jobs/runs/submit \
            -H "Authorization: Bearer $DATABRICKS_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{
              \"run_name\": \"CI Run\",
              \"existing_cluster_id\": \"${{ steps.create-cluster.outputs.cluster_id }}\",
              \"spark_jar_task\": {
                \"main_class_name\": \"runners.TestRunner\",
                \"parameters\": [\"--REPORT_PATH\", \"${{ env.REPORT_PATH }}\"]
              },
              \"libraries\": [
                { \"jar\": \"dbfs:/FileStore/jars/amida-databricks-1.0-SNAPSHOT-jar-with-dependencies.jar\" }
              ]
            }")
          run_id=$(echo "$run_response" | jq -r '.run_id')
          echo "::set-output name=run_id::$run_id"

      - name: Poll for Job Completion (max 2 mins)
        run: |
          for i in {1..4}; do
            status=$(curl -s -X GET "$DATABRICKS_HOST/api/2.1/jobs/runs/get?run_id=${{ steps.trigger-job.outputs.run_id }}" \
              -H "Authorization: Bearer $DATABRICKS_TOKEN" | jq -r '.state.life_cycle_state')
            echo "Attempt $i: Job status = $status"
            if [[ "$status" == "TERMINATED" || "$status" == "SKIPPED" || "$status" == "INTERNAL_ERROR" ]]; then
              break
            fi
            sleep 30
          done

      - name: Fetch Artifacts Back to GitHub (if available)
        run: |
          DBFS_PATH="/SharedResults/${{ steps.timestamp.outputs.path }}"
          echo "Attempting to download results from $DBFS_PATH..."
          for i in {1..5}; do
            databricks fs cp dbfs:$DBFS_PATH/cucumber.json ./cucumber.json --overwrite && break || sleep 10
          done

      - name: Upload Cucumber JSON to GitHub Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cucumber-json
          path: ./cucumber.json
          retention-days: 7

      - name: Download Cucumber Artifact
        uses: actions/download-artifact@v4
        with:
          name: cucumber-json

      $1
      $1
      - name: Transition Jira Test Execution to Done
        run: |
          TRANSITION_ID=31  # Replace with actual ID if needed
          curl -X POST \
            -H "Authorization: Bearer KqYgtEnBjmKbGTXyc8idSTE505y4TaG3tF3Xxy" \
            -H "Content-Type: application/json" \
            --data '{
              "transition": { "id": "'$TRANSITION_ID'" }
            }' \
            https://jira.devops.va.gov/rest/api/2/issue/SDP-37817/transitions

      - name: Cleanup SharedResults Directory (optional)
        if: success()
        run: |
          DBFS_PATH="/SharedResults/${{ steps.timestamp.outputs.path }}"
          echo "Cleaning up $DBFS_PATH..."
          databricks fs rm -r dbfs:$DBFS_PATH || echo "Nothing to clean."

      - name: Cleanup Feature Files and Push
        if: success()
        run: |
          echo "üßπ Cleaning up feature files and pushing to repo..."
          rm -rf ./src/main/resources/features/*
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git add src/main/resources/features/
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è No changes to commit after cleanup."
          else
            git commit -m "üßº Cleanup: removed feature files after execution"
            git push origin feature/Rudra-Graph

      - name: Terminate Databricks Cluster
        if: always()
        run: |
          echo "üõë Terminating Databricks cluster..."
          curl -s -X POST "$DATABRICKS_HOST/api/2.0/clusters/delete" \
            -H "Authorization: Bearer $DATABRICKS_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"cluster_id\": \"${{ steps.create-cluster.outputs.cluster_id }}\"}"
